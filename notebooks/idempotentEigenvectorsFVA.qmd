---
Author: "FVA"
Title: "Idempotent Eigenvectors"
Date: "2023-10-01"
engine: julia
julia:
  exeflags: ["--project=.."]
theme:
    light: flatly
    dark: darkly
---

# Environment Construction

```{julia}
using DrWatson
DrWatson.quickactivate(@__DIR__)
#@quickactivate(@__DIR__)
#using Pkg; Pkg.add("CSV")
using DataFrames, CSV
using Graphs
#using Semifields
using SparseArrays
using MaxPlus
using Printf
```

Now define attributes for the environment:
```{julia}
set_tropical_display(1)#Q:FVA: where does this primitive come from?
Base.show(io::IO, ::MIME"text/latex", x::MP) = show(io, MIME"text/plain", x)
Base.show(io::IO, ::MIME"text/latex", A::MPAbstractVecOrMat) = show(io, MIME"text/plain", A)
```

Fix the random seed:

```{julia}
using Random
seed_value = 17
Random.seed!(seed_value)
```

A general tolerance values:

```{julia}
tol = 1e-10#Should also be used for comparisons in function `is approx`.
```
# FVA's approach

## Data 
### Example data 1

A simple two-scc example to obtain the eigenvectors supplied by ERFs: 

```{julia}
M2 = [
    0.0  1.0  -Inf -Inf -Inf;
   -Inf 0.0  2.0  -Inf -Inf;
    1.0 -Inf 0.0  -Inf -Inf;
   -Inf -Inf -Inf 0.0 1.0;
   -Inf -Inf -Inf -Inf 0.0
]
A2 = MP(M2)
```
This matrix is in UFNF2.

This has two strongly connected components:
  * [[1,2,3]]: 
    *  single irreducible component by cycle: 1 -> 2 -> 3 -> 1 lambda = 4/3
  * [[4], [5]]: t
    * two irreducible scc: 5 -> 5: lambda = 0, 4 -> 4: lambda = 0.0
    * Condensation digraph [4] -> [5] So, 5 is shadowed, but has the same lambda as the original. 

This has two distinct and orthogonal eigenspaces in coords [1,2,3] and [4,5]
  * the first one is a total order of ingfinite length generated by any of v_1, v_2, or v_3 which are also linearly ordered. 
  * the second one is also a total order of length 2: ev_4 < ev_5 
So the whole space is a product of two orders.

We obfuscate the structure to validate :
```{julia}
using Permutations#Check for interface at: https://juliapackages.com/p/permutations
p2 = RandomPermutation(size(M2,1))#keep everything contained there
A2obfuscated = MP(M2[p2.data, p2.data])
```

We extract a matrix in UFNF0 (irreducible) for tests:
```{julia}
M0 = M2[1:3,1:3]
A0 = MP(M0)
```
There is really no need for obfusction of irreducible matrices. 


### Example data 2 from Akian, Bapat, Gaubert, 2015. Chapter 25: Max-plus Algebra

This is a reducible matrix in UFNF1 with a single connected component and shadowed classes:

```{julia}
M1 = fill(-Inf, (8,8))
M1[1,1:8] = [0 -Inf 0 -Inf 7 -Inf -Inf -Inf]
M1[2:4, 2:4] = [-Inf 3 0; 1 -Inf -Inf; 2 -Inf -Inf]
M1[4,8] = 10
M1[5:7, 5:7] = [1 0 -Inf; -Inf -Inf 0; -1 2 -Inf]
M1[7,8] = 23
M1[8,8] = 0
A1 = MP(M1)
```
Permute rows and columns to obfuscate the UFNF2

```{julia}
#using Permutations#Check for interface at: https://juliapackages.com/p/permutations
p1 = RandomPermutation(size(M1,1))#keep everything contained there
A1obfuscated = MP(M1[p1.data, p1.data])
```

### Example 3: A matrix in UFNF2 proper

```{julia}
n1 = size(M1,1)
n2 = size(M2,1)
M3 = fill(-Inf, (n1 + n2, n1 + n2))
M3[1:n1, 1:n1] = M1
M3[n1 .+ (1:n2), n1 .+ (1:n2)] = M2
A3 = MP(M3)#Three block diagnonal components => a product space of three dimensions!
```

An obfuscated version:

```{julia}
#using Permutations#Check for interface at: https://juliapackages.com/p/permutations
p3 = RandomPermutation(size(M3,1))#keep everything contained there
A3obfuscated = MP(M3[p3.data, p3.data])
```
### Example 3: A matrix that properly accepts UFNF3 form

```{julia}
# Two empty rows and columns, 1 empty col and 3 empty rows 
n_ι = 2#both empty rows and cols
n_α = 3#only empty cols
n_β = size(M3,1)
n_ω = 1#only empty rows 
#n = n_ι + n_α + n_β + n_ω
M4 = fill(-Inf, (n_ι + n_α + n_β + n_ω, n_ι + n_α + n_β + n_ω))
M4[ (n_ι + n_α) .+ (1:n_β),(n_ι + n_α) .+ (1:n_β)] = M3#Two empty rows, Three empty cols at the beginning, prior to M3
#M4[(2 + 1 + n1) .+ (1:n2), (2 + n1) .+ (1:n2)] = M2#1 empty line prior to 
M4[n_ι .+ (1:n_α), n_ι + n_α .+ (1:(n_β + n_ω))] = rand(vcat([-Inf, -Inf, -Inf], collect(0:4)), n_α, n_β + n_ω)# Random generation of isolated links. Unrefined and inelegant.
M4[(n_ι + n_α) .+ (1:n_β), (n_ι + n_α + n_β) .+ (1:n_ω)] = rand(vcat([-Inf, -Inf, -Inf], collect(0:4)), n_β, n_ω)
A4 = MP(M4)
```

An obfuscated version:

```{julia}
#using Permutations#Check for interface at: https://juliapackages.com/p/permutations
p4 = RandomPermutation(size(M4,1))#keep everything contained there
A4obfuscated = MP(M4[p4.data, p4.data])
```
## Computations

### Basic matrices

These matrices are used time and again:

```{julia}
using MaxPlus
#using LinearAlgebra
"""
Maxplus identity

"""
function I(n::Int, m::Int)::Matrix{MP}
    #(n > 0 && m > 0)  || error("Can only generate n,m >= 1 matrices")   
    return MP([(i != j ? -Inf : 0.0) for i in 1:n, j in 1:m])#this is In, but we Avoid allocating extra
    #return Matrix{MP}(MaxPlus.I, n, m)#FVA: from the documentation on constructions
    #return eye(MP,n,m)
end 
#LinearAlgebra.I(n) = I(n,m)
I(n) = I(n,n)

"""
Maxplus bottom matrix of order `n x m`. With single parameter the square matrix is created.
"""
function nB(n::Int, m::Int)
    #(n > 0 && m > 0) ||  error("Can only generate n,m >= 1 matrices")   
    return fill(ε, (n,m))#FVA: from the documentation on constructions varepsilon = ε is the bottom
    
end
nB(n::Int) = nB(n,n)

"""
Maxplus top matrix of order `n x m`. With single parameter the square matrix is created.
"""
function nT(n::Int, m::Int)
    #(n > 0 && m > 0) ||  error("Can only generate n,m >= 1 matrices")   
    return fill(mptop,(n,m))#FVA: from the documentation on constructions, mptop is Inf
end
nT(n::Int) = nT(n,n)
```

Testing for approximation we need to extend Base.isapprox

```{julia}
dist = 1e-10
Base.isapprox(a::MP, b::MP) = 
    isapprox(plustimes(a) .+ dist, plustimes(b) .+ dist)
a = Tropical{MaxPlus.Max}(5.0)
b = Tropical{MaxPlus.Max}(5.0 + 1e-10)
@assert a ≈ b
#Base.isapprox(A::Matrix{MP}, B::Matrix{MP}) = all(plustimes(A) .+ dist .≈ plustimes(B) .+ dist)
Base.isapprox(A::Matrix{MP}, B::Matrix{MP}) = plustimes(A) .+ dist ≈ plustimes(B) .+ dist#Not the poinwise comparison
A = MP(M1)
#@assert A  ≈ A .* MP(dist)#THis is a failure of approximation. Check Semifields package.
```

### Basic operation: closures

Fast MP-exponentiation of matrices using the doubling approach. 

```{julia}
function mmp_mpower_raw(A::Matrix{MP}, n::Int64, k::Int64)
     # PReconditions: size(A,1) == size(A, 2), k >= 0
    #k == 0 && return I(n)#early termination 
    k == 1 && return A#early termination
    #Y = [MP(i != j ? -Inf : 0.0) for i in 1:n, j in 1:n]#this is In, but we Avoid allocating extraA
    Y::Matrix{MP} = I(n)
    #Here n > 1
    #Here we have: #Y = I_n#in the first pass
    while k > 0
        if isodd(n)
            Y = Y * A 
        end 
        A = A * A
        k >>= 1#Bitwise division by 2
    end
    return Y#when k==0 it returns just Y = I(n), when k==1 it would pass through exactly once.
end
```

```{julia}
mmp_mpower_raw(A0, 3, 20)
mmp_mpower_raw(A0, 3, typemax(Int64)) #.>= typemax(Float64)
```
```{julia}
"""
    mmp_mpower(A::Matrix{MP}, k::Int) → Matrix{MP}

A function to work out the `k`-th MP power of matrix `A` by the power doubling method.

This function is the basis for the closures so it needs to be extremely efficient.

Parameters
  * `A` the square matrix to be exponentiated
  * `k` the non-negative exponent 

Result: `A^k`

#Example:
"""
function mmp_mpower(A::Matrix{MP}, k::Int64)
    m,n = size(A)
    m == n || error("Can only find k-th power on a square matrix.")
    k < 0 && error("Can only do positive powers by this method.")
    return mmp_mpower_raw(A, n, k)
end

```

Some tests:

```{julia}
#A = MP(M1)
mmp_mpower(A0,20)
```

Using the fact (from fact 2 of Gaubert and Akian, 2015), that: A^* = I ++ A^1 ++ · · · ++ A^n−1.
And due to the idempotency of addition we have (I ++ A)^(n-1) = I ++ A^1 ++ · · · ++ A^n−1.


```{julia}
"""
    trclosure(A::Matrix{MP}) → Matrix{MP}

Transitive reflexive closure aka Kleene's star of matrix `A` over the max-plus idempotent semifield.

Result: `A^*`. This is not a syntax for it. Instead use `trclosure(A)`.
Remember the transitive closure behaves as `A^+ = A * A^* = A^* * A`.
"""
function trclosure(A::Matrix{MP})
    m,n = size(A)
    m == n || error("Can only find trclosure on a square matrix.")
    n == 1 && return (A[n,n] > 0 ? [mptop;;] : [mpe;;])#Might correctly diverge to the top!
    return trclosure_raw(A::Matrix{MP}, n)
end 
trclosure_raw(A::Matrix{MP},n) = mmp_mpower(I(n) + A, n-1)
 
"""
    tclosure(A::Matrix{MP}) → Matrix{MP}

Transitive closure---aka Kleene's plus `A^+` of a matrix `A` over the max-plus idempotent semifield.

Result: `A^+`. This is not a syntax for it. Instead use `tclosure(A)`.

Remember the transitive reflexive closure behaves as `A^* = In + A^+` whhere `In` is the max-plus identity of order n.
"""
function tclosure(A::Matrix{MP})
    m,n = size(A)
    m != n && error("Can only find trclosure on a square matrix.")
    n == 1 && return ((A[n,n] > 0 ? [mptop;;] : [mpe;;]) * A)
    return tclosure_raw(A::Matrix{MP}, n)    
end
tclosure_raw(A::Matrix{MP}, n) = A * trclosure_raw(A,n)

#= Naive implementation
function trclosure(A::Matrix{MP})
    n = size(Aᵢ, 1)
    I_n = [i == j ? MP(0.0) : MP(-Inf) for i in 1:n, j in 1:n]#FVA: uses too much space?
    # FVA: actually by idempotency of addition this only needs to be done at most until Bp^n
    # FVA: this is the naive, iterative algorithm. 
    Ai = deepcopy(A)#Will store the latest A^i. Here i=1
    Aplus = MP[-Inf] .+ Ai #MP[-Inf for _ in 1:n, _ in 1:n]#Zero max-plus matrix. Will accumulate
    for i in 2:n#FVA: at least goes over this once, since n==1 is taken care of.
        Ai = Ai * A#FVA: maxplus addition The copy is done in the first update
        Aplus = Aplus + Ai#FVA: maxplus addition
    end
    # Step 3: Work out A^* = I_n + A^+
    #FVA: for A^* we just add the identity to A^+

    return Aplus + I_n
end
=#
```

Example on the test data testing the relationships between the tclosure and the trclosure:

```{julia}
A = A0
#closures do not converge for main lambda bigger than 0.0
r = howard(sparse(A))
Anorm = A ./ r.eigenvalues[1]
n = size(Anorm,1)
tclosure(Anorm)
#In = I(n,n)#FVA: old, cumbersome mode
#In = [MP(i != j ? -Inf : 0.0)  for i in 1:n, j in 1:n]#FVA: old, cumbersome mode
@assert trclosure(Anorm) .+ eps(Float64) ≈ I(n) + tclosure(Anorm) .+ eps(Float64)
#@assert plustimes(trclosure(Anorm)) ≈ plustimes(In + tclosure(Anorm))#FVA: old, cumbersome mode
@assert tclosure(Anorm) ≈ Anorm * trclosure(Anorm)
#@assert plustimes(tclosure(Anorm)) ≈ plustimes(Anorm * trclosure(Anorm))#FVA: old, cumbersome mode
#k = 100
#A = Anorm
#Next debug mmp_mpower
B = fill(MP(2.0), (1,1)) #MP.(ones(1,1)*2)
@assert trclosure(B) == tclosure(B) == [mptop;;]#Special case!
```

### The base case: irreducible matrices

A primitive to deal with the base case. 

```{julia}
"""
    getUtMV₀_raw(Aᵢ::Matrix{MP}, n::Int; tol=1e-10) → 

Precondition: Aᵢ is irreducible, square and siz(Aᵢ,1)=n > 1

This is only supposed to be used if in costly computations. It is better to use
getUtMV₀(Aᵢ::Matrix{MP}; tol=1e-10) (q.v.) for most computations. 
"""
function getUtMV₀_raw(Aᵢ::Matrix{MP}, n::Int; tol=1e-10)
     #n = size(Aᵢ, 1)
    #I_n = [i == j ? MP(0.0) : MP(-Inf) for i in 1:n, j in 1:n]#FVA: uses too much space?

    # Otherwise, 
    # 0.find the eigenvalue: 
    # a) using Howard's algorithm
    r = howard(sparse(Aᵢ))
    λᵢ = r.eigenvalues[1]#There is always at least one eigenvalue.
    # b) using the maximal cycle mean (pending)

    # 1: Normalize Aᵢ dividiendo por autovalor (max-plus: restar)
    A = Aᵢ ./ λᵢ#Normalized matrix right MP division (unless this is the top aka Inf in Maxplus)

    #= 
    # Paso 2: Calcular A^+ = A + A^2 + ... until convergence
    # FVA: actually by idempotency of addition this only needs to be done at most until Bp^n
    # FVA: this is the naive, iterative algorithm. 
    Ai = deepcopy(A)#Will store the latest A^i. Here i=1
    Aplus = MP[-Inf] .+ Ai #MP[-Inf for _ in 1:n, _ in 1:n]#Zero max-plus matrix. Will accumulate
    for i in 2:n#FVA: at least goes over this once, since n==1 is taken care of.
        Ai = Ai * A#FVA: maxplus addition The copy is done in the first update
        Aplus = Aplus + Ai#FVA: maxplus addition
    end
    # Step 3: Work out A^* = I_n + A^+
    #FVA: for A^* we just add the identity to A^+
    Aᵢstar = Aplus + I_n
    =#
    Aᵢstar = trclosure(A)
    # FVA: it can be improved by doing a doubling algorithm, see in old Matlab code. 
    # (I + A)*(I + A) = I + A + A^2 by idempotency.

    #Instead of using the eigenodes detected by the Howard algorithm, detect them here: 
    evNodes = findall(i -> abs(Aᵢstar[i, i]) < tol, 1:n)#FVA: could we use approx here?s
    #evNodes = findall(i -> Aᵢstar[i,i] ≈ 0.0, 1:n)#FVA: type incorrect

    return λᵢ, evNodes, Aᵢstar[evNodes,:], Aᵢstar[:,evNodes]
end 

"""
    getUtMV₀(Aᵢ::Matrix{MP}; tol=1e-10) -> λᵢ, evNodes, Aᵢstar
 
Finds out the UtMV form of the eigenvector theorem for an irreducible matrix. 

Precondition: `Aᵢ` is an irreducible matrix. 

Returns:
- `λᵢ`: the single eigenvalue for this matrix
- `evNodes`: a vector of vectors of nodes where left and right eigenvectors are realised
- `levs`: left eigenvectors. 
- `revs`: right eigenvectors.
Examples:
'''julia
a = 1
'''
"""
function getUtMV₀(Aᵢ::Matrix{MP}; tol=1e-10)
    (n,m) = size(Aᵢ)
    n == m || n == 0 | error("Can only do UFN0 for non-zero dimension square matrices.")
    #Early termination. Probably wrong if Bi = [Inf]
    n == 1 && return Aᵢ[1,1], [n], I(n), I(n)
    return getUtMV₀_raw(Aᵢ, n; tol=tol)
end
```

An example on a part of the demo matrix which is irreducible:

```{julia}
#cc = [1,2,3]
Aᵢ = A0
λᵢ, evNodes, levs, revs  = getUtMV₀(A0)
#Check left and right eigenvector properties.
# Check the left eigenvector property: we have to use approx due to calculations errors
@assert levs * Aᵢ ≈ levs .* λᵢ#Julia is so elegant!
#@assert plustimes(levs * Aᵢ) ≈ plustimes(levs .* λᵢ)#Old cumbersome mode
# Check the right eigenvector property: we have to use approx due to calculations errors
@assert Aᵢ * revs ≈ revs * λᵢ#product by a scalar is lifted to pointwise multiplication!
#@assert plustimes(Aᵢ * revs) ≈ plustimes( revs .* λᵢ)
# Check for a dimension 1 matrix.
B = fill(MP(2.0), (1,1)) #MP.(ones(1,1)*2)
@assert getUtMV₀(B) == (2.0, [1], I(1), I(1))
```

SO FAR 11/07/2025

### Properly reducible matrices

These accept an UFNF1 form:

```{julia}
"""
    getUtMV₁_raw(Aᵣ::Matrix{MP}, n::Int; tol=1e-10)

An auxiliary function to carry out 
"""
function getUtMV₁_raw(Aᵣ::Matrix{MP}, n::Int; tol=1e-10)
    #n = size(Aᵣ)
    dg = Graphs.DiGraph(Aᵣ .!= MP(-Inf))#Consider it a digraph#Make this generic in semiring
    sccs = strongly_connected_components(dg)#Find the strongly connected components of dg
    cdg = Graphs.condensation(dg, sccs)# And their condensation digraph.
    #terminal case: only one scc: this must be irreducible, so we can find everything
    nsccs = length(sccs)#number of scc of cc of index s
    lambdas = Array{Array{MP}}(undef, nsccs)#Make room for eigenvalues
    #Astars = Array{Matrix{MP}}(undef, nsccs)#Make room for stars
    evNodes = Array{Array{Int}}(undef, nsccs)#Make room for eivenvector indices. 
    levs = Array{Array{Matrix{MP}}}(undef, nsccs)
    revs = Array{Array{Matrix{MP}}}(undef, nsccs)
    # Finding the UFNF1 form
    order = (nsccs == 1 ? collect(1:nsccs) : topological_sort_by_dfs(cdg))
    sccs = sccs[order]#Reordering in topological order produces UFN1 for this matrix!
    reorderedNodes = reduce(vcat,sccs)
    Aᵣreordered = nsccs == 1 ? Aᵣ : Aᵣ[reorderedNodes, reorderedNodes]#Biggest mat to deal with
    #@assert isUFN1(Aᵣreordered)#FVA: this predicate does not exist. 
    if nsccs == 1#it is already in UFNF0, no need to consult cdg
        # The whole A_s matrix has 1 eigenvalue
        lambdas[1], distalEvNodes, distalLevs, distalRevs  = getUtMV₀(Aᵣ[sccs[1], sccs[1]])
        evNodes[1] = sccs[1][distalEvNodes]
        levs[1] = [distalLevs]#trclosure already done!
        revs[1] = [distalRevs]#trclosure already done!
    else#we may need to reorder the condensation digraph topologically. 
        #Proceed by topological sort of the condensation digraph
        #order = topological_sort_by_dfs(cdg)
        #localSccs = sccs[order]
        for c in 1:nsccs
            nodes = sccs[c]#Nodes are local to this component
            lambdas[c], distalEvNodes, distalLevs, distalRevs  = getUtMV₀(Aᵣ[nodes, nodes])
            evNodes[c] = nodes[distalEvNodes]
            Aᵣordered_norm = Aᵣordered ./ lambdas[c]
            Aᵣordered_norm_star = trclosure(Aᵣordered_norm)#too brutish and not needed. 
            #evNodes = findall(i -> abs(Aᵣordered_norm_star[i, i]) < tol, 1:n)#already found in evNodes[c]
            levs[c] = [Aᵣordered_norm_star[evNodes[c],:]]
            revs[c] = [Aᵣordered_norm_star[:,evNodes[c]]]     
        end
    end
    #cg and evNodes tells how to reorder this component in UFNF2 form
    return dcg, lambdas, evNodes, levs, revs
end

"""
    getUtMV₁(Aᵣ::Matrix{MP}; tol=1e-10) → 

Find the UtMV of a matrix that is reducible and find its UFNF1 form. 

Precondition:
    - only for square matrices of dimension > 0
    - no empty rows or columns, e.g. already done UFNF2 analysis, block diagonal form.

Returns:
- `cg`: condensation digraph of the reducible component. 
- `λᵢ`: the single eigenvalue for this matrix
- `evNodes`: a vector of vectors of nodes where left and right eigenvectors are realised
- `levs`: left eigenvectors. 
- `revs`: right eigenvectors.
"""
function getUtMV₁(Aᵣ::Matrix{MP}; tol=1e-10)
    (n,m) = size(Aᵣ)
    n == m || n == 0 | error("Can only do UtMV1 for non-zero dimension square matrices.")
    return getUtMV₁_raw(Aᵣ,n; tol=tol)
end

```

Observing the strongly connected components and their digraphs:

```{julia}
Aᵣ = A1obfuscated#From Akian, Bapat, Gaubert 2015
dg = Graphs.DiGraph(Aᵣ .!= MP(-Inf))
nv(dg)
sccs = strongly_connected_components(dg)#Find the strongly connected components of dg
cdg = Graphs.condensation(dg, sccs)#Their condensation digraph. Does not keeep the sccs
order = topological_sort_by_dfs(cdg)
mySccs = sccs[order] 
myNodes = reduce(vcat,mySccs)
A1obfuscated[myNodes,myNodes]##Not exactly the original one, but a reordering equates them. 
#Topological sorting of the cond 
#using GraphMakie
using GraphPlot
#p = graphplot(dg, ilabels=collect(vertices(dg)))
nodelabel = collect(vertices(cond))
gplot(cond; 
    nodelabel=sccs,
    title="condensaton digraph of A2 obfuscated")
```


Two examples one a degenerate case and another a proper triangular.

```{julia}

```

### Properly block diagonal matrices

```{julia}
function getUFNF₂(A::Matrix{MP})
    # Find the weakly connected components
    dg = Digraphs.Graph(A .!= MP(-Inf))
    ccs = weakly_connected_components(dg)
    #Asym = A + transpose(A)#symmetrization of matrix.
    #g = Graphs.Graph(Asym .!= MP(-Inf))
    #ccs = connected_components(g)#List of connected components: can appear in any order, 

    #but it is nice to order them in increasing number of nodes per component, to make the UFNF "stable by the base"
    sort!(ccs; by=length)#here we used the in-sort type of sorting, since the inportant info are the node identities.

    # UFNF2 diagonal of UFNF1
    nccs = length(ccs)#number of connected components
    #All the following vectors are coindexed with ccs
    lambdas = Array{Array{MP}}(undef, nccs)#Make room for eigenvalues
    Astars = Array{Matrix{MP}}(undef, nccs)#Make room for stars
    evnodes = Array{Array{Int}}(undef, nccs)#Make room for eivenvector indices. 
    conds = Array{Any}(undef,nccs)#Condensation digraphs of connected components
    for s in 1:nccs#go over each connected component
        #caveat: the node numbers below are relative to those in cc (1st level of indirection)
        println("Finding UFNF for cc:", ccs[s])
        lnodes = ccs[s]
        lambdas[s], evnodes_s, Astars_s, conds_s = getUFNF₁(A[lnodes,lnodes])#FVA: Traslate nodes!
        #@assert evs are ok here.
    end
    return ccs, conds, lambdas, evnodes, Astars
end
```

```{julia}
A = MP.(M)#FVA: entry point for parameter of the algorithm
ccs,tconds,tlambdas,tevnodes,tAstars = getUFNF₂(A)
neworder = reduce(vcat,ccs)#Reordering of the nodes for an UFNF2
A[neworder,neworder]
```

## Generic matrices

If A has zero lines it can be transformed by a simultaneous row and column permutation of V A into UFNF3 as per (5):

```{julia}
function getUFNF₃(A::Matrix{MP})
    (n,m) = size(A)
    n = m || error("UFNF3 only applicable to square matrices")#strictly not true
    allZeroRows = all(A[i,:] == ε)
    #All(iszero, S, dims=2)#suggested by stack overflow. Recall: @assert iszero(ε)
```


## Data

REad the functional connectome data for C. elegans interneurons from a CSV file and convert it to a matrix of Semifields.
```{julia}
# Leer el archivo CSV
df = CSV.read(
        datadir("exp_raw","functional_connectivity_interneurons.csv"), 
        DataFrame)
#TODO: FVA. Explore the dataset. rows? Cols?

# Nombres de neuronas
neuron_labels = Vector{String}(df[:, 1]);  # Asume que la primera columna del DataFrame contiene los nombres

# Reemplazar valores missing con -Inf
df_filled = coalesce.(df, -Inf);

# Convertir el DataFrame en una matriz numérica y la transpone con '
mat = Matrix(df_filled[:, 2:end])'  # la primera columna son las etiquetas

# Asegurar que está en el tipo correcto
mat = convert(Array{Float64}, mat)

# Reconstruimos el df transpuesto
# Crear el DataFrame con los datos transpuestos
df_transposed = DataFrame(mat, copy(neuron_labels))  # Usa neuron_labels como nombres de columna

# Añadir la columna con los nombres de fila
df_transposed = insertcols(df_transposed, 1, :Neuron => neuron_labels)

#Cambiar los 0s por -Inf
#mat_mp_ready = map(x -> x == 0.0 ? -Inf : x, mat)
B = MP.(mat);
# Imprime la matriz MP completa sin truncar
show(stdout, "text/plain", B)
```

Now do a test check:

```{julia}
neuronas = ["AIBL", "RIAR", "RIAL"]
row_idxs = findall(n -> n in neuronas, neuron_labels)
test = df[row_idxs, neuronas]
show(stdout, "text/plain", test)
show(stdout, "text/plain",B[row_idxs,row_idxs])#FVA: not in UFNF0
```
# Test Case 1: Working out the right eigenvalues

Find the strongly connected components by their matrices.
```{julia}
"""
    extract_all_strong_components → submatrices, components, node_to_component, condensation

- condensation: condensation digraph

A primitive to extract all strong components from a matrix in UFNF2 format
with no zero lines. 

This returns the condensation digraph of the matrix, 

TODO: Change to entries in a semifield or CSemifield. 
"""
function extract_all_strong_components(B_mp::Matrix{MP})
    # FVA: this is the digraph associated to the matrix.
    n = size(B_mp, 1) 
    @assert n == size(B_mp, 2)#FVA: only valid for square matrices, in principle!
    #G = Graphs.DiGraph(n)    
    #You have to start using comprehensions instead of for loops
    #[add_edge!(G,i,j) for i = 1:n, j=1:n if B_mp[i, j] != MP(-Inf)]#This needs to be changed to the bottom of the semifield.
    #FVA: you can cleanly create the digraph from an interator on the edges
    G2 = Graphs.DiGraph(Edge.([(i,j) for i = 1:n, j=1:n if B_mp[i, j] != MP(-Inf)]))#Comprehension to create the edges of the graph.
    G = Graphs.DiGraph(B_mp .!= MP(-Inf))#FVA: Even easier, you can use the matrix directly to create the graph.
    #FVA: why a {59, 193} directed simple Int64 graph, and not a Bool graph. A: because that would be a non-generalisable encoding of link weights! Anyway, it should be either ones or tops, but the latter are difficult to operate with 
    @assert G == G2 #FVA: this is a test to see if the graph is well built.
    #for i in 1:n
    #    for j in 1:n
    #        if B_mp[i, j] != MP(-Inf)
    #            add_edge!(G, i, j)
    #        end
    #    end
    #end

    #This is building the direct and inverse dictionaries of nodes and components. 
    # 1. this is the component => node dictionary.
    components = Graphs.strongly_connected_components(G)

    # FVA: some constructors are immediate!
    #node_to_component = Dict(enumerate(components))
    #@assert length(components) == length(node_to_component)
    # Creamos el mapeo manualmente
    #node_to_component = Dict{Int, Int}()
    node_to_component = zeros(Int64,1,n) 
    #FVA: since every node belongs to just one component, it is better to use a vector
    #node_to_component = zeros(Int, n)#make space
    # Las submatrices de componentes
    submatrices = Matrix{MP}[]
    #=
    component_nodes = Vector{Int}[]
    for nodes in node_groups
        push!(submatrices, B_mp[nodes, nodes])
        push!(component_nodes, nodes)
    end
    =#
    for (comp_id, comp_nodes) in enumerate(components)
        push!(submatrices, B_mp[comp_nodes, comp_nodes])
        for node in comp_nodes
            node_to_component[node] = comp_id
        end
    end
    @assert length.(components) == map(m -> size(m,1), (submatrices))
    @assert all()

    
    # node_groups: lista de nodos por componente
    # FVA: this is a list of lists, where each sublist contains the nodes in a component.
    # FVA: but this seems to be just the list of nodes by component, which is exactly `components
    #=
    num_components = length(components)
    node_groups = [Int[] for _ in 1:num_components]
    for (node, comp_id) in sort(collect(node_to_component))
        push!(node_groups[comp_id], node)
    end
    =#


    # Condensed graph: it is better to give it the clue of the components
    #cond = Graphs.condensation(G)
    cond = Graphs.condensation(G, components)#If you have the components, you can build the condensed matrix directly.

    #return submatrices, component_nodes, cond, node_groups
    return submatrices, components, node_to_component, cond
end
```

Use this primitive to extract all strong components from the matrix `B_mp`:

```{julia}
submatrices, components, node_to_component, cond = extract_all_strong_components(B_mp)
# Print the number of components
println("Number of components: ", length(components))
# Print the nodes in each component
for (i, nodes) in enumerate(components)
    println("Component $i: ", join(nodes, ", "))
end
# Print the condensed graph
println("Condensed graph: ", cond)
# Print the submatrices
for (i, submat) in enumerate(submatrices)
    println("Submatrix for component $i:\n", submat)
end
# Print the node groups
for (i, nodes) in enumerate(node_to_component)
    println("Node group $i: ", join(nodes, ", "))
end
```

A better principled visualisation after Emma:
```{julia}
for (i, (B_sub, nodes)) in enumerate(zip(submatrices, components))
    println("==== Componente $i ====")
    println("Nodos:")
    println(nodes)
    println("\nSubmatriz:")
    # Imprime la matriz MP completa sin truncar
    show(stdout, "text/plain", B_sub)
    println("\n\n---------------------------\n")
end
```

## Calculo de autovalores

Work out the eigenvalues

```{julia}
#FVA: dimension this better
eigenvalues = MP.(zeros(Float64,n,1))
for (i, B_sub) in enumerate(submatrices)
    r = howard(sparse(B_sub)) 
    lambda = r.eigenvalues  
    println("Autovalor max-plus de componente $i: ", MP(lambda[1]))
    #push!(eigenvalues, MP(lambda[1]))
    eigenvalues[i] = MP(lambda[1])
end
```

Find out the eigenvectors for irreducible matrices:

```{julia}
"""
all_eigenvectors(Au::Matrix{MP}, λ ::MP;  tol=1e-10) → evPositions, Anstar

A primitive to work out the left (rows) and right (column) eigenvalues of an irreducible matrix
by looking at the elements of the diagonal of the TSR closure of the normalized matrix.
- A: square MP matrix. 
- λ: its eigenvector. 

We return the nodes rather than the actual vectors. 
- evPositions: positions of the eigenvectors in 1:n with n = size(A,1)
- Anstar: star of normalized input matrix A

TODO: this algorithm is formally identical for MinPlus analysis, eg. polymorphic. 
"""
function allEigenvectors(Aᵢ::Matrix{MP}, λᵢ::MP; tol=1e-10)
    n = size(Aᵢ, 1)
    I_n = [i == j ? MP(0.0) : MP(-Inf) for i in 1:n, j in 1:n]#FVA: uses too much space?
    n == 1 && return n, I_n#Early termination. Probably wrong if Bi = [-Inf]

    # Paso 1: Normalizar B_i dividiendo por autovalor (max-plus: restar)
    A = Aᵢ ./ λᵢ#Normalized matrix right MP division (unless this is the top aka Inf in Maxplus)

    # Paso 2: Calcular A^+ = A + A^2 + ... until convergence
    # FVA: actually by idempotency of addition this only needs to be done at most until Bp^n
    # FVA: this is the naive, iterative algorithm. 
    # FVA: it can be improved by doing a doubling algorithm, see in old Matlab code. 
    Ai = deepcopy(A)#Will store the latest A^i. Here i=1
    Aplus = MP[-Inf] .+ Ai #MP[-Inf for _ in 1:n, _ in 1:n]#Zero max-plus matrix. Will accumulate
    for i in 2:n#FVA: at least goes over this once, since n==1 is taken care of.
        Ai = Ai * A#FVA: maxplus addition The copy is done in the first update
        Aplus = Aplus + Ai#FVA: maxplus addition
    end
    # Step 3: Work out A^* = I_n + A^+
    #FVA: for A^* we just add the identity to A^+
    Astar = Aplus + I_n

    #ERF
    #=
    A_star = deepcopy(A)
    A_prev = MP[-Inf for _ in 1:n, _ in 1:n]
    A_star .= max.(A_star, I_mp)  # A_star = max(A_star, I)
    max_iter = 100
    iter = 0
    while !isequal(A_star, A_prev) && iter < max_iter
        iter += 1
        A_prev = deepcopy(A_star)
        A_star = max.(A_star, A_prev * Bp)
    end
    =#

    # Paso 3: Detectar posiciones con valor cercano a 0 en la diagonal
    evPositions = findall(i -> abs(Astar[i, i]) < tol, 1:n)

    # Paso 4: Autovectores por la izquierda: filas en posiciones relevantes
    #left_eigenvectors = [Astar[:, i] for i in evPositions]

    # Paso 5: Autovectores por la derecha: columnas en posiciones relevantes
    #right_eigenvectors = [Astar[i, :] for i in evPositions]

    #FVA: it were better to return the index over the A_star
    #return left_eigenvectors, right_eigenvectors, Astar
    return evPositions,Astar
end
```

Check with the matrices from classes 8(2 nodes) and 9(24 columns):

```{julia}
begin
    i = 1
    evPositions, Astar = allEigenvectors(submatrices[i], eigenvalues[i])
    i = 8
    evPositions, Astar = allEigenvectors(submatrices[i], eigenvalues[i])
    i = 9
    evPositions, Astar = allEigenvectors(submatrices[i], eigenvalues[i])
end
```

## Upper Frobenius Normal Form (UFNF)

```{julia}
"""
    getUFNF(B::Matrix{MP}, node_groups::Vector{Vector{Int}}, cond::DiGraph) → 

A primitive to obtain the permutation that transforms a given possibly reducible matrix into 
upper Frobenius Normal Form (UFNF). This normal forms favours that starting classes in the 
condensed digraph appear to the upper left and final classes to the lower right. This would indeed be the case for a single Strongly Connected Component (SCC).

The algorithm proceeds in the hierarchy of normal forms, except it supposes there are no 
empty rows or columns (so UFNF3 is not needed.)

Check wrt "The Spectra of Reducible Matrices over Completed Commutative Idempotent Semiﬁelds and their Spectral Lattices", by Valverde-Albacete and Peláez Moreno

"""
function getUFNF(B::Matrix{MP}, 
    node_groups::Vector{Vector{Int}}, 
    cond::DiGraph, 
    neuron_labels::Vector{String})

    # UFNF2: diagonal matrix of SCCs of symmetrical graph. 
    num_sccs = length(node_groups)
    group_indices = collect(1:num_sccs)

    # Paso 1: obtener CGCs como componentes débilmente conectadas del DAG cond
    undirected_cond = Graph(cond)  # convierte el DAG en grafo no dirigido
    cgc_components = connected_components(undirected_cond)  # cada CGC es un conjunto de SCCs

    # Paso 2: ordenar CGCs por número de SCCs (de menor a mayor)
    sorted_cgcs = sort(cgc_components, by = cgc -> length(cgc))

    # Paso 3: obtener orden topológico global de las SCCs
    global_topo = topological_sort(cond)
    topo_pos = Dict(scc => findfirst(isequal(scc), global_topo) for scc in group_indices)

    # Paso 4: ordenar SCCs dentro de cada CGC por orden topológico
    ordered_sccs = Vector{Int}()
    for cgc in sorted_cgcs
        ordered = sort(cgc, by = scc -> topo_pos[scc])
        append!(ordered_sccs, ordered)
    end

    # Paso 5: expandimos a nodos reales
    ordered_nodes = reduce(vcat, node_groups[ordered_sccs])
    ordered_labels = neuron_labels[ordered_nodes]
    B_frobenius = B[ordered_nodes, ordered_nodes]

    return B_frobenius, ordered_nodes, ordered_labels
end
```

